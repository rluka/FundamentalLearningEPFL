{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u2F37pU70wA"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exo4wW7570wF"
      },
      "source": [
        "# On the previous episode...\n",
        "\n",
        "On the previous lab we introduced neural network and implemented them using the `PyTorch` library. Our experiments showed that for image classification a CNN architecture yields good results on MNIST and, depending on the complexity of the network you created, \"good\" results on CIFAR10. However, dealing with MNIST someone might have gotten the wrong impression: \"everythings work out-of-the-box or like magic in Deep Learning\". Reality is not so rosy and we must go to great lengths do replicate our success on MNIST for other datasets.\n",
        "\n",
        "In this lab, we will explore common pitfalls as well as common tips and tricks to resolve them. These simple methods will provide superior performance and are very easy to incorporate in our pipeline. \n",
        "\n",
        "Specifically, we will talk about:\n",
        "- Batch Normalization\n",
        "- Learning rate scheduler\n",
        "- Residual Connections\n",
        "\n",
        "In the next lab, we will see more cool \"tricks\". \n",
        "\n",
        "So... let's get started!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Umg4I_nr70wI"
      },
      "outputs": [],
      "source": [
        "# first we load all the necessary libraries\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from training_utils import fit, predict, visualize_images, plot_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qY10beEa70wJ"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# load the train dataset: CIFAR10\n",
        "# YOUR CODE GOES HERE\n",
        "\n",
        "# load the test dataset: CIFAR10\n",
        "# YOUR CODE GOES HERE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JPV3xAk70wM",
        "outputId": "90af77af-9199-47e7-8177-a47bec5b38fb"
      },
      "outputs": [],
      "source": [
        "# define the hyperparameters\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "# find out which device is available\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im4pPN_q70wM"
      },
      "outputs": [],
      "source": [
        "train_dataloader = # YOUR CODE GOES HERE\n",
        "\n",
        "\n",
        "test_dataloader = # YOUR CODE GOES HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp7ESZ0D70wN"
      },
      "source": [
        "Now, let's visualize some samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "s_K_vHwD70wN",
        "outputId": "38d98baa-2347-4b6b-ddbb-7d93f21eef4e"
      },
      "outputs": [],
      "source": [
        "visualize_images(train_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVJqoRx7sQ0H"
      },
      "source": [
        "At first glance, we can see that this dataset is far more complex than MNIST."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGyceXYl70wP"
      },
      "source": [
        "# Batch normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keETaVfyhTHo"
      },
      "source": [
        "We want to learn fast and converge at the same time. If we use a small learning rate, we will converge but it will be too slow. On the other hand, if we use large learning rate, our training will become inconsistent and we will bounce all over the place and never converge. Additionaly, higher learning rates  cause exploding or vanishing gradients i.e. the phenomenon where the multiplication of gradients via the chain rule induces a compound effect on the lower layers, preventing them from learning.\n",
        "\n",
        "Can we have the best of both worlds? Enter **Batch Normalization**.\n",
        "\n",
        "1. What does BatchNorm aims to solve? We want to\n",
        "    * avoid unstable gradients,\n",
        "    * allow faster learning rates leading to faster convergence,\n",
        "    * reduce the effect of initialization.\n",
        "\n",
        "2. Why does BatchNorm actually do?\n",
        "    * Suppose we are given values of $x$ over a mini-batch $\\mathcal{B}=\\{x_i\\}_{i=1}^m$. Our goal is to learn some parameters $\\gamma$ and $\\beta$ that perform the proper scaling.\n",
        "\n",
        "    * First, we compute the mini-batch mean\n",
        "    $$\n",
        "    \\mu_{\\mathcal{B}}=\\frac{1}{m}\\sum_{i=1}^mx_i\n",
        "    $$\n",
        "    * and mini-batch variance\n",
        "    $$\n",
        "    \\sigma^2_{\\mathcal{B}}=\\frac{1}{m}\\sum_{i=1}^m (x_i-\\mu_{\\mathcal{B}})^2\n",
        "    $$\n",
        "    * we use these quantities to normalize our input\n",
        "    $$\n",
        "    \\hat{x}_i=\\frac{x_i-\\mu_{\\mathcal{B}}}{\\sqrt{\\sigma^2_{\\mathcal{B}}+\\epsilon}}\n",
        "    $$\n",
        "    * We scale, shift and return the output\n",
        "    $$\n",
        "    y_i=\\gamma \\hat{x}_i+\\beta\\equiv \\text{BN}_{\\gamma, \\beta}(x_i)\n",
        "    $$\n",
        "    * Essentially, for each mini-batch we normalize the inputs by subtracting their mean and dividing by their standard deviation (estimated based on the statistics of the current mini-batch)  \n",
        "\n",
        "\n",
        "3. Why does BatchNorm work?\n",
        "\n",
        "    * BatchNorm is widely used (e.g. the original paper [1] has over 30000 citations). However, the reasons of its success are not perfectly clear.\n",
        "    * The original authors claim that BatchNorm helps alleviate *Internal Covariate shift*, i.e. the phenomenon of shifting input distributions. Specifically, the input to each layer can be seen as a data distribution that the layer is trying to “learn”. The model, though, does not see the whole dataset but simply mini-batches. If this distribution stays consistent across batches, the layer can \"learn effectively\".  But, does this happen in practice?\n",
        "    * the reality is that different mini-batches have different statistics, e.g. mean, variance etc, making the input distribution to the layers jump around. In other words, the input distribuion shifts for every mini-batch. We are trying to learn a \"moving target\". What if we stabilize it?\n",
        "    * Batch normalization keeps the input normalized (duh!), preventing them from becoming too large or small and keeping the distribution consistent. \n",
        "    \n",
        "    * It also directly placates the exploding/vanishing gradient problem and  allows higher learning rates.\n",
        "\n",
        "    * However, other explanations have been proposed. [2] claims that BatchNorm \"makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training\".\n",
        "\n",
        "\n",
        "---\n",
        "[1] S. Ioffe and C. Szegedy, “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,” in Proceedings of the 32nd International Conference on Machine Learning, Jun. 2015, pp. 448–456. Accessed: Oct. 25, 2021. [Online]. Available: https://proceedings.mlr.press/v37/ioffe15.html\n",
        "\n",
        "[2] S. Santurkar, D. Tsipras, A. Ilyas, and A. Madry, “How Does Batch Normalization Help Optimization?,” in Advances in Neural Information Processing Systems, 2018, vol. 31. Accessed: Oct. 25, 2021. [Online]. Available: https://papers.nips.cc/paper/2018/hash/905056c1ac1dad141560467e0a99e1cf-Abstract.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8muCisTyEz4Q"
      },
      "outputs": [],
      "source": [
        "class CNN_Cifar10(nn.Module): \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # We use a Sequential, i.e. the inputs passes through each of\n",
        "        # the modules below, one-by-one\n",
        "        self.conv = nn.Sequential(         \n",
        "            nn.Conv2d(\n",
        "                in_channels=3,              \n",
        "                out_channels=16,            \n",
        "                kernel_size=3,              \n",
        "                stride=1,                   \n",
        "                padding=1,                  \n",
        "            ),                              \n",
        "            nn.ReLU(),                      \n",
        "            nn.MaxPool2d(kernel_size=2), \n",
        "            nn.Conv2d(\n",
        "                in_channels=16, \n",
        "                out_channels=32, \n",
        "                kernel_size=3, \n",
        "                stride=1, \n",
        "                padding=1),     \n",
        "            nn.ReLU(),                      \n",
        "            nn.MaxPool2d(2),    \n",
        "        )\n",
        "              \n",
        "        # fully connected layer, output 10 classes\n",
        "        self.out = nn.Linear(..., 10)       \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = ...\n",
        "        x = self.out(x)\n",
        "        return x   \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eOovZfM2GNzT",
        "outputId": "b34de0dd-b374-4d64-ac72-e73f6b62dee9"
      },
      "outputs": [],
      "source": [
        "cnn = CNN_Cifar10().to(DEVICE)\n",
        "optimizer = torch.optim.SGD(cnn.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "plot_loss(\n",
        "    fit(\n",
        "        cnn,\n",
        "        train_dataloader = train_dataloader,\n",
        "        optimizer = optimizer,\n",
        "        epochs = 50,\n",
        "        device = DEVICE\n",
        "    )\n",
        ")\n",
        "\n",
        "predict(\n",
        "    cnn,\n",
        "    test_dataloader = test_dataloader,\n",
        "    device = DEVICE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZxZnhC9VYdM"
      },
      "outputs": [],
      "source": [
        "# Add BatchNorm layers to the previous model.\n",
        "\n",
        "class CNNwithBN(nn.Module): \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # We use a Sequential, i.e. the inputs passes through each of\n",
        "        # the modules below, one-by-one\n",
        "        # YOUR CODE GOES HERE\n",
        "              \n",
        "        # fully connected layer, output 10 classes\n",
        "        self.out = nn.Linear(2048, 10)       \n",
        "        \n",
        "    def forward(self, x):\n",
        "        # YOUR CODE GOES HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wvX2kUDhV3Th",
        "outputId": "9738750f-bd87-46b1-e9ef-7d1a7347da66"
      },
      "outputs": [],
      "source": [
        "cnn2 = CNNwithBN().to(DEVICE)\n",
        "optimizer = torch.optim.SGD(cnn2.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "plot_loss(\n",
        "    fit(\n",
        "        cnn2,\n",
        "        train_dataloader = train_dataloader,\n",
        "        optimizer = optimizer,\n",
        "        epochs = 50,\n",
        "        device = DEVICE\n",
        "    )\n",
        ")\n",
        "\n",
        "predict(\n",
        "    cnn2,\n",
        "    test_dataloader = test_dataloader,\n",
        "    device = DEVICE\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myX7KAesWeaF"
      },
      "source": [
        "One of the benefits of Batch Norm is that it allows us to use higher learning rates. Adapt the code above to do so. Does the model learn faster?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vt1fMrPIEp1e",
        "outputId": "51b70a28-e80d-410e-97eb-f99ebc485998"
      },
      "outputs": [],
      "source": [
        "cnn2 = CNNwithBN().to(DEVICE)\n",
        "optimizer = torch.optim.SGD(cnn2.parameters(), lr=..., momentum=0.9)\n",
        "\n",
        "plot_loss(\n",
        "    fit(\n",
        "        cnn2,\n",
        "        train_dataloader = train_dataloader,\n",
        "        optimizer = optimizer,\n",
        "        epochs = 50,\n",
        "        device = DEVICE\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "predict(\n",
        "    cnn2,\n",
        "    test_dataloader = test_dataloader,\n",
        "    device = DEVICE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning rate scheduler\n",
        "\n",
        "We have just seen that batch normalization allows, in this case, for a quicker improvement with a higher learning rate. Still, the loss plateaus quickly and me start seeing minimal improvement. This is often due to the optimization algorithm overshooting the gradient descent.\n",
        "\n",
        "Can we reduce the step size on the go? Yes! :smiley: This is what learning schedulers are for. The idea is simple: instead of a constant learning rate, we reduce it based on some conditions, or after a certain amount of steps.\n",
        "\n",
        "Two common schedulers are [`MultiStepLR`][MultiStepLR] and [`ReduceLROnPlateau`][ReduceLROnPlateau]. The first one, simply multiplies our learning rate `lr` by a constant factor $\\gamma < 1$ after some predefined number of steps. For instance, if the initial learning rate is `lr=1`, and we set $\\gamma=0.5$ for \"milestones\" of 20 and 50, then the optimizer is going to use `lr=1` for the first 20 epochs, then `lr=0.5` for the subsequent 30, and finally `lr=0.25` for all the remaining ones.\n",
        "\n",
        "`ReduceLROnPlateau` tracks a given metric, e.g. validation loss or accuracy, and reduces the learning rate if no improvement is seen after a predefined number of steps, called \"patience\".\n",
        "\n",
        "[MultiStepLR]: https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiStepLR.html\n",
        "[ReduceLROnPlateau]: https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following cell, augment the `fit` function from previous weeks (in `training_utils.py`) to accept a scheduler argument and use it while training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from training_utils import train_epoch\n",
        "\n",
        "def fit_scheduler(\n",
        "    model: nn.Module,\n",
        "    train_dataloader: DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    epochs: int,\n",
        "    device: torch.device,\n",
        "    scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None\n",
        "):\n",
        "    losses = []\n",
        "    # YOUR CODE GOES HERE\n",
        "    return losses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now train again the batch-norm-cnn, using the `MultiStepLR` scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn2 = CNNwithBN().to(DEVICE)\n",
        "optimizer = torch.optim.SGD(cnn2.parameters(), lr=0.05, momentum=0.9)\n",
        "\n",
        "scheduler = ...\n",
        "\n",
        "plot_loss(\n",
        "    fit_scheduler(\n",
        "        cnn2,\n",
        "        train_dataloader = train_dataloader,\n",
        "        optimizer = optimizer,\n",
        "        epochs = 50,\n",
        "        device = DEVICE,\n",
        "        scheduler=scheduler,\n",
        "    )\n",
        ")\n",
        "\n",
        "predict(\n",
        "    cnn2,\n",
        "    test_dataloader = test_dataloader,\n",
        "    device = DEVICE\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-jpcEJU70wP"
      },
      "source": [
        "# Residual connections\n",
        "\n",
        "As neural networks go deeper, they are able to construct complex representations and yield superior performance. However, we cannot simply stack as many layers as we want to increase the depth. \n",
        "\n",
        "![caption](media/resnet-no-skip-horizontal.png)\n",
        "\n",
        "This is due to the **vanishing gradient** problem. Specifically, backpropagating the gradient to earlier layers involves repeated multiplication (with small values) rendering the gradient extremely small. This effectively means that as we go deeper, performance gets saturated. Instead of improved performance we even have degradation!\n",
        "\n",
        "How can we reconcile this tradeoff? On the one hand, we want to increase depth but on the other hand this hurts convergence. \n",
        "\n",
        "Enter **skip connections** [3]! The network of the previous figure now becomes the following:\n",
        "\n",
        "![caption](media/resnet-horizontal.png)\n",
        "\n",
        "Now, let's think why these skip connections work. First, they allow the gradient to flow via this shortcut connection, which helps mitigate the problem of vanishing gradient. Second, they allow the model to learn the identity function. In other words, this ensures that the higher layer will perform at least as good as the lower layer.\n",
        "\n",
        "---\n",
        "[3] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image Recognition,” in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, Jun. 2016, pp. 770–778. doi: 10.1109/CVPR.2016.90.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoeJeLQ8bZMi"
      },
      "source": [
        "First, we build the network of the first image, i.e. with no skip connections. The Resnet depicted above is characterized by an interesting pattern. It consists of \"super-blocks\" (see the different colors) and each one consists of two blocks that start after one residual connection and finish just before one. Notice that each color is associated with a different number, i.e. 64, 128, 256, 512. \n",
        "\n",
        "We will build a `nn.Module` for each block and repeat it to create the super-blocks and by extension the whole architecture.\n",
        "\n",
        "The ResNet depicted above is meant to be used for `ImageNet`, a more complex dataset compared to `CIFAR10`. For computational considerations, we amend our implementation and make a simpler version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "626cjZVwa7HD"
      },
      "outputs": [],
      "source": [
        "class WrongBlock(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels = in_planes,\n",
        "            out_channels = planes, \n",
        "            kernel_size=3, \n",
        "            stride=stride, \n",
        "            padding=1, \n",
        "            bias=False)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            planes, \n",
        "            planes, \n",
        "            kernel_size=3,\n",
        "            stride=1, \n",
        "            padding=1, \n",
        "            bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes \n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W2AoqF0ra7HG",
        "outputId": "ab22ca68-6bdd-41da-ede1-2beca8bd9aa9"
      },
      "outputs": [],
      "source": [
        "# initialize the model \n",
        "model = ResNet(block=..., num_blocks=[2,2,2,2]).to(DEVICE)\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# train the ResNet\n",
        "plot_loss(\n",
        "    fit(\n",
        "        model,\n",
        "        train_dataloader = train_dataloader,\n",
        "        optimizer = optimizer,\n",
        "        epochs = 30,\n",
        "        device = DEVICE\n",
        "    )\n",
        ")\n",
        "\n",
        "# predict with the trained model\n",
        "predict(\n",
        "    model,\n",
        "    test_dataloader = test_dataloader,\n",
        "    device = DEVICE\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQHjjbs2fEQz"
      },
      "source": [
        "How many layers does the above model have?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCbe7tlbbRvM"
      },
      "source": [
        "Now, we add skip connections. Notice that sometimes the skip connection cannot be simply an identity function, since the dimensions will not match. Identify the condition when this is necessary. In that case, the shortcut function should be a convolution followed by BatchNorm. \n",
        "\n",
        "Fill the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBEe9QiT70wQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CorrectBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # YOUR CODE GOES HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W2AoqF0ra7HG",
        "outputId": "ab22ca68-6bdd-41da-ede1-2beca8bd9aa9"
      },
      "outputs": [],
      "source": [
        "# initialize the model \n",
        "model = ResNet(block=..., num_blocks=[2,2,2,2]).to(DEVICE)\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# train the ResNet\n",
        "plot_loss(\n",
        "    fit(\n",
        "        model,\n",
        "        train_dataloader = train_dataloader,\n",
        "        optimizer = optimizer,\n",
        "        epochs = 30,\n",
        "        device = DEVICE\n",
        "    )\n",
        ")\n",
        "\n",
        "# predict with the trained model\n",
        "predict(\n",
        "    model,\n",
        "    test_dataloader = test_dataloader,\n",
        "    device = DEVICE\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lhdhcjdehm6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Deep Learning Tips and Tricks-orig.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('nikdim')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "03a8ff65f077f05ff345d067c3fc612a41150fb0cbc9d89e43af938917f7b052"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
