{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdbLD14fsnFc",
    "tags": []
   },
   "source": [
    "# Homework 3 - Luka RadiÄ‡\n",
    "## SCIPER: 354502"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25eNQUQEtRY9",
    "tags": []
   },
   "source": [
    "## Import the libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "f31vHJl-tTvg"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nin18ZUfttp0",
    "tags": []
   },
   "source": [
    "## Exercise 1: Backpropagation with logistic loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "46G2sLaquudg"
   },
   "outputs": [],
   "source": [
    "# Defining the constants used throughout the exercise\n",
    "D = 5\n",
    "K = 6\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "def grad_sigmoid(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: `predict` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "A7qgZ-6itzz1"
   },
   "outputs": [],
   "source": [
    "def predict(X,W):\n",
    "    X_0 = X\n",
    "\n",
    "    W_1 = W[\"w_1\"]\n",
    "    w_2 = W[\"w_2\"]\n",
    "\n",
    "    Z_1 = X_0 @ W_1\n",
    "    X_1 = sigmoid(Z_1)\n",
    "    z_2 = X_1 @ w_2\n",
    "    y_hat = sigmoid(z_2)\n",
    "\n",
    "    return Z_1, z_2, y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: `logistic_loss` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss(y, y_hat):\n",
    "    eps = 1e-12\n",
    "    log_loss = -y.dot(np.log(y_hat+eps)) - (1-y).dot(np.log(1-y_hat+eps))\n",
    "    return np.mean( log_loss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average log-loss of the batch is -5.000444502909205e-12\n"
     ]
    }
   ],
   "source": [
    "B = 5 # esentially it can be anything, the result does not depend on it\n",
    "y = np.zeros(B)\n",
    "y_hat = np.zeros(B)\n",
    "log_loss = logistic_loss(y, y_hat)\n",
    "print(\"The average log-loss of the batch is {}\".format(log_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: `stable_logistic_loss` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_logistic_loss(y, z_2):\n",
    "    log_loss = y.dot(np.logaddexp(0,-z_2)) + (1-y).dot(np.logaddexp(0,z_2))\n",
    "    return np.mean( log_loss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average stable-log-loss of the batch is 0.0\n"
     ]
    }
   ],
   "source": [
    "B = 5 # esentially it can be anything, the result does not depend on it\n",
    "y = np.zeros(B)\n",
    "z_2 = -1e10 * np.ones(B)\n",
    "stable_log_loss = stable_logistic_loss(y, z_2)\n",
    "print(\"The average stable-log-loss of the batch is {}\".format(stable_log_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 4: Analytical backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us denote with:\n",
    "- $x_i^{(0)}$ the $i$-th component of the input vector\n",
    "- $w_{ij}^{(1)}$ the weight of the edge connecting $x_i^{(0)}$ and the $j$-th node in the hidden layer $z_j^{(1)}$\n",
    "- $w_j^{(2)}$ the weight of the edge connecting $z_j^{(1)}$ and the output node $z^{(2)}$\n",
    "- $\\hat{y}$ the final prediction\n",
    "\n",
    "where $i=\\overline{1,D},\\; j=\\overline{1,K}$. Note that the inputs of activation functions for each node in the NN can be calculated as:\n",
    "$$\n",
    "z_j^{(1)} = \\sum_{i=1}^D \\; w_{ij}^{(1)} x_i^{(0)}, \\quad z^{(2)} = \\sum_{j=1}^K \\; w_j^{(2)} \\sigma\\left(z_j^{(1)}\\right)\n",
    "$$\n",
    "We use the stable implementation of logistic loss:\n",
    "$$\n",
    "\\mathcal{L}(x,y,w) = y \\log\\left( 1 + e^{-z^{(2)}} \\right) + (1-y) \\log\\left( 1 + e^{z^{(2)}} \\right)\n",
    "$$\n",
    "\n",
    "We first find the partial derivatives of *hidden-output* weights:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\mathcal{L}(x,y,w)}{\\partial w_j^{(2)}} &= - y \\frac{e^{-z^{(2)}}}{1 + e^{-z^{(2)}}} \\frac{\\partial z^{(2)}}{\\partial w_j^{(2)}} + (1-y) \\frac{e^{z^{(2)}}}{1 + e^{z^{(2)}}} \\frac{\\partial z^{(2)}}{\\partial w_j^{(2)}} \\\\\n",
    "    &= \\left[ - y \\frac{e^{-z^{(2)}}}{1 + e^{-z^{(2)}}} + (1-y) \\frac{e^{z^{(2)}}}{1 + e^{z^{(2)}}} \\right] \\; \\sigma\\left(z_j^{(1)}\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now we find the partial derivatives of *input-hidden* weights. For that purpose, it is useful to rewrite $z^{(2)}$ as:\n",
    "$$\n",
    "z^{(2)} = \\sum_{j=1}^K \\; w_j^{(2)} \\sigma\\left(z_j^{(1)}\\right) = \\sum_{j=1}^K \\; w_j^{(2)} \\sigma\\left(\\sum_{i=1}^D \\; w_{ij}^{(1)} x_i^{(0)}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\mathcal{L}(x,y,w)}{\\partial w_{ij}^{(1)}} &= \\left[ - y \\frac{e^{-z^{(2)}}}{1 + e^{-z^{(2)}}} + (1-y) \\frac{e^{z^{(2)}}}{1 + e^{z^{(2)}}} \\right] \\frac{\\partial z^{(2)}}{\\partial w_{ij}^{(1)}} \\\\\n",
    "    &= \\left[ - y \\frac{e^{-z^{(2)}}}{1 + e^{-z^{(2)}}} + (1-y) \\frac{e^{z^{(2)}}}{1 + e^{z^{(2)}}} \\right] \\; w_j^{(2)} \\sigma'\\left(\\sum_{i=1}^D \\; w_{ij}^{(1)} x_i^{(0)}\\right) x_i^{(0)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Considering that\n",
    "$$\n",
    "\\frac{d\\sigma(x)}{dx} = \\frac{e^{-x}}{(1+e^{-x})^2} = \\frac{1}{1+e^{-x}} \\left(\\frac{1+e^{-x}}{1+e^{-x}}-\\frac{1}{1+e^{-x}}\\right) = \\sigma(x)\\left[1-\\sigma(x)\\right]\n",
    "$$\n",
    "we can finally write the partial derivative as\n",
    "$$\n",
    "    \\frac{\\partial \\mathcal{L}(x,y,w)}{\\partial w_{ij}^{(1)}} = \\left[ - y \\frac{e^{-z^{(2)}}}{1 + e^{-z^{(2)}}} + (1-y) \\frac{e^{z^{(2)}}}{1 + e^{z^{(2)}}} \\right] \\; w_j^{(2)} \\sigma\\left(z_j^{(1)}\\right)\\left[1-\\sigma\\left(z_j^{(1)}\\right)\\right] x_i^{(0)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: `gradient` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, y, W):\n",
    "    B = X.shape[0]\n",
    "    # Feedforward\n",
    "    Z_1, z_2, y_hat = predict(X, W)\n",
    "    X_1 = sigmoid(Z_1)\n",
    "    # Backpropagation\n",
    "    delta_2 = -y * np.exp(-z_2)/(1+np.exp(-z_2)) + (1-y)*np.exp(z_2)/(1+np.exp(z_2))\n",
    "    delta_w_2 = delta_2.reshape(-1,1) * X_1\n",
    "    delta_w_2_mean = np.mean(delta_w_2, axis=0)\n",
    "    delta_1 = delta_2.reshape(-1,1) * grad_sigmoid(Z_1) * W[\"w_2\"]\n",
    "    delta_w_1_mean = 1/B * X.T @ delta_1\n",
    "    \n",
    "    return {\n",
    "        \"w_1\": delta_w_1_mean,\n",
    "        \"w_2\": delta_w_2_mean\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0.3,0.3,0.3,0.3],[0.3,0.3,0.3,0.3],[0.01,0.2,0.01,0.3],[0.3,0.3,0.3,0.3]])\n",
    "W = {\n",
    "    \"w_1\": np.ones((4, 5)),\n",
    "    \"w_2\": np.ones(5)\n",
    "}\n",
    "y = np.ones(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97901263 0.97901263 0.9583431  0.97901263]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "expected = 0.93244675427215695\n",
    "_, _, yours = predict(X, W)\n",
    "print(yours)\n",
    "print(np.sum((yours - expected) ** 2) < 1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w_1': array([[-0.0008644 , -0.0008644 , -0.0008644 , -0.0008644 , -0.0008644 ],\n",
      "       [-0.00132708, -0.00132708, -0.00132708, -0.00132708, -0.00132708],\n",
      "       [-0.0008644 , -0.0008644 , -0.0008644 , -0.0008644 , -0.0008644 ],\n",
      "       [-0.0015706 , -0.0015706 , -0.0015706 , -0.0015706 , -0.0015706 ]]), 'w_2': array([-0.01862824, -0.01862824, -0.01862824, -0.01862824, -0.01862824])}\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "expected = {\n",
    "    'w_1': np.array([\n",
    "        [ -1.06113639e-05,  -1.06113639e-05,  -1.06113639e-05, -1.06113639e-05,  -1.06113639e-05],\n",
    "        [ -2.12227277e-05,  -2.12227277e-05,  -2.12227277e-05, -2.12227277e-05,  -2.12227277e-05],\n",
    "        [ -3.18340916e-05,  -3.18340916e-05,  -3.18340916e-05, -3.18340916e-05,  -3.18340916e-05],\n",
    "        [ -4.24454555e-05,  -4.24454555e-05,  -4.24454555e-05, -4.24454555e-05,  -4.24454555e-05]]),\n",
    "    'w_2': np.array(\n",
    "        [-0.00223387, -0.00223387, -0.00223387, -0.00223387, -0.00223387])\n",
    "}\n",
    "yours = gradient(X,y,W)\n",
    "print(yours)\n",
    "print( np.sum(\n",
    "    [np.sum((yours[key] - expected[key]) ** 2) for key in expected.keys()]) < 1e-15 )\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
