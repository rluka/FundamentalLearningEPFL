{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you will learn today**: This lab serves as an introduction to PyTorch. We will learn the different steps required in training a deep learning model with modern libraries, such as PyTorch.\n",
    "\n",
    "So, which are these steps?\n",
    "\n",
    "* Preliminaries:\n",
    "    * load the train and test datasets, `train_dataset` and `test_dataset` (MNIST in our case)\n",
    "    * turn the datasets into a \"dataloaders\": `train_dataloader` and `test_dataloader`\n",
    "    * define your `model` architecture\n",
    "    * define your `optimizer`, e.g. SGD\n",
    "\n",
    "\n",
    "* Training: Now we have all the building blocks and we need to make our model \"learn\". In most cases, the training follows a specific \"recipe\". Specifically, we feed the `model` the whole `train_dataset` using batches that come from the `train_dataloader`. We repeat this a certain number of times, called `epochs`. Each epoch consists of `batches`. So what do we do for each batch?\n",
    "    * zero out the optimizer. In essence we prepare the optimizer for the incoming data\n",
    "    * compute the output of the model $f(\\cdot)$ for our current data: $x\\mapsto f(x)$\n",
    "    * compute the loss: $\\mathcal{L}(f(x), y)$ where $y$ denotes the ground truth\n",
    "    * perform the `backpropagation` algorithm which involves computing the gradients and performing the update rule\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the preliminaries out of the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we load all the necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the datasets. We are going to work with MNIST and our goal is classify digits. This is a popular dataset and PyTorch offers it out-of-the-box, making our life easy! We simply need to call the corresponding method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data are given as PIL images. We need to convert our data to a type \n",
    "# that is readable by a Neural Network. Thus, we use the ToTensor() \"transform\" \n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    # T.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# load the train dataset\n",
    "# Hint: look at \n",
    "# 1. https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "# 2. https://pytorch.org/vision/stable/datasets.html\n",
    "train_dataset = ... # INSERT YOUR CODE HERE\n",
    "\n",
    "# load the test dataset\n",
    "test_dataset = ... # INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hyperparameters\n",
    "BATCH_SIZE = 1024\n",
    "TEST_BATCH_SIZE = 2048\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "# find out which device is available\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we cannot use the whole dataset; it is too large for computers to handle. Instead, we perform *stochastic* gradient descent, i.e. we feed the model part of the data called batches. In order to do so, we use Pytorch DataLoaders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the dataloader for the traininig dataset. \n",
    "# Here we shuffle the data to promote stochasticity.\n",
    "train_dataloader = ... # INSERT YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Construct the dataloader for the testing dataset.\n",
    "test_dataloader = ... # INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize some samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get the first 10 images of the train dataset. Hint: use next(), iter()\n",
    "images = ... # YOUR CODE GOES HERE\n",
    "grid = torchvision.utils.make_grid(images, nrow=5, padding=10)\n",
    "\n",
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "\n",
    "show(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to define our model. We will start with a simple model, a MultiLayer Perceptron (MLP) with 2 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        # define the different modules of the network\n",
    "        super().__init__()\n",
    "        # How many features should our model have?\n",
    "        # self.fc1 = YOUR CODE GOES HERE\n",
    "\n",
    "        # How many outputs should our model have?\n",
    "        # self.fc2 = YOUR CODE GOES HERE\n",
    "        # we also define the non-linearity \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # You should (a) transform the a size that is readable\n",
    "        # by the MLP and (b) pass the the input x successively \n",
    "        # through the layers.\n",
    "        # ***************************************************s\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "model = Net()\n",
    "\n",
    "# move model to device\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# define the optimizer\n",
    "# Hint: https://pytorch.org/docs/stable/optim.html\n",
    "optimizer = optim.SGD(..., lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define:\n",
    "* the `fit` function that performs the training part\n",
    "* the `predict` function that takes as input the test dataloader and prints the performance metrics (e.g. accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: nn.Module, \n",
    "    train_dataloader: DataLoader, \n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    device: torch.device\n",
    "    ):\n",
    "    '''\n",
    "    This function implements the core components of any Neural Network training regiment.\n",
    "    In our stochastic setting our code follows a very specific \"path\". First, we load the batch\n",
    "    a single batch and zero the optimizer. Then we perform the forward pass, compute the gradients and perform the backward pass. And ...repeat!\n",
    "    '''\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        # move data and target to device\n",
    "        # INSERT YOUR CODE HERE\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        # INSERT YOUR CODE HERE\n",
    "\n",
    "        # do the forward pass\n",
    "        output = # INSERT YOUR CODE HERE\n",
    "\n",
    "        # compute the loss\n",
    "        loss = # INSERT YOUR CODE HERE\n",
    "\n",
    "        # compute the gradients\n",
    "        # INSERT YOUR CODE HERE\n",
    "\n",
    "\n",
    "        # perform the gradient step\n",
    "        # INSERT YOUR CODE HERE\n",
    "\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(train_dataloader.dataset)\n",
    "\n",
    "\n",
    "def fit(\n",
    "    model: nn.Module, \n",
    "    train_dataloader: DataLoader, \n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    epochs: int, \n",
    "    device: torch.device):\n",
    "    '''\n",
    "    the fit method simply calls the train_epoch() method for a \n",
    "    specified number of epochs.\n",
    "    '''\n",
    "\n",
    "    # keep track of the losses in order to visualize them later\n",
    "    # Train for numerous epochs:\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = ... # INSERT YOUR CODE HERE\n",
    "        print(f\"Epoch {epoch}: Loss={running_loss}\")\n",
    "        losses.append(running_loss)\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model: nn.Module, test_dataloader: DataLoader, device: torch.device):\n",
    "    # INSERT YOUR CODE HERE\n",
    "\n",
    "    print(f'Test set: Avg. loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_dataloader.dataset)} ({accuracy:.0f}%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform a \"sanity check\". Our model is at the moment initialized randomly and we have 10 classes (each class has approximately the same number of samples). This means that we should get random performance -> ~10% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(model=model, test_dataloader=test_dataloader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for 10 epochs\n",
    "losses = fit(\n",
    "    # INSERT YOUR CODE HERE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the loss progression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss progression across epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\n",
    "    # INSERT YOUR CODE HERE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not very good. There are some major problems. We see from the plot above that the loss keeps dropping and does not \"plateau\". This indicates that we can run the optimization a few more epochs and improve the performance. Another point is that our learning rate is too sloww or the selection of vanilla SGD as our optimizer is not optimal. In the next section we will see that simply changing the optimizer (from SGD to Adam) yields very different results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the MLP does not take into account the nature of images: close pixels convey local information that is important. Using an MLP, we do not have the notion of the \"pixel neighbourhood\". We, therefore, neglect important information with an MLP. There are however models better suited for vision problems, such as Convolutional Neural Networks or CNNs.\n",
    "\n",
    "With the code structure we have created, we can simply define a CNN and test its performance quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # define a CNN with 2 convolutional layers, followed by ReLU and Maxpool each, \n",
    "        # and a fully connected layer at the end.\n",
    "        # Hint: use nn.Sequential()\n",
    "        # INSERT YOUR CODE HERE\n",
    "              \n",
    "        # fully connected layer, output 10 classes\n",
    "        # INSERT YOUR CODE HERE\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # INSERT YOUR CODE HERE \n",
    "        return x   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "cnn = CNN().to(DEVICE)\n",
    "\n",
    "# define the optimizer. Use Adam\n",
    "optimizer = # INSERT YOUR CODE HERE \n",
    "\n",
    "# train the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does the CNN perform compared to the MLP?\n",
    "# INSERT YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: play with CIFAR10\n",
    "\n",
    "MNIST is a fairly simple dataset. What happens in more challenging datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('playground')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b6258ac5f9ce9aee8bc7e8f09e746bac739d42425ff8343fe5df569d2e6cb19"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
